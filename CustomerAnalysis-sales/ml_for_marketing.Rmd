---
title: "ML_marketing"
output: html_document
---

# Machine Learning for Marketing Analytics in R
https://www.datacamp.com/courses/marketing-analytics-in-r-statistical-modeling

```{r}
rm(list = ls()) # cleans the memory

library(tidyverse)
library(corrplot)
library(rms)
library(MASS)
library(SDMTools)
library(boot)
library(stats)
```

```{r import}
salesData <- read.csv('salesData.csv')
salesData2_4 <- read.csv('salesDataMon2To4.csv')
defaultData <- read.csv('defaultData.csv', sep=";")
dataNextOrder <- read.csv('survivalDataExercise.csv', sep=',')
newsData <- get(load('newsData.RData'))

```

## Modeling Customer Lifetime Value with Linear Regression

```{r, eval = FALSE}
# Visualization of correlations
#salesData %>% select_if(is.numeric) %>%
#  select(-id) %>%
#  cor() %>% 
#  corrplot()

# Frequent stores
ggplot(salesData) +
    geom_boxplot(aes(x = mostFreqStore, y = salesThisMon))
  
# Preferred brand
ggplot(salesData) +
    geom_boxplot(aes(x = preferredBrand, y = salesThisMon))
```


```{r linear regression, eval= FALSE}
# Model specification using lm
salesSimpleModel <- lm(salesThisMon ~ salesLast3Mon, 
                       data = salesData)
# Looking at model summary
summary(salesSimpleModel)
```

```{r multireg, message = FALSE}

salesModel1 <- lm(salesThisMon ~ . - id, data = salesData) # multiregression
vif(salesModel1) # Checking variance inflation factors


salesModel2 <- lm(salesThisMon ~ . - id - preferredBrand - nBrands, data = salesData) # Estimating new model by removing information on brand
vif(salesModel2) # Checking variance inflation factors

predSales5 <- predict(salesModel2, newdata = salesData2_4)
mean(predSales5)
```


## Logistic Regression for Churn Prevention

```{r EDA defaultData}
summary(defaultData) # Summary of data
str(defaultData) # Look at data structure

# Analyze the balancedness of dependent variable
ggplot(defaultData,aes(x = PaymentDefault)) +
  geom_histogram(stat = "count") 
```


```{r logreg defaultData}

# Build logistic regression model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
                   age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + 
                   billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + 
                   payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, 
                 family = binomial, data = defaultData)


summary(logitModelFull) # Take a look at the model

coefsexp <- coef(logitModelFull) %>% exp() %>% round(2) # extract the coefficients and transform them to the odds ratios.
coefsexp
```

The stepAIC() function gives back a reduced model,includes only main effects for the predicted variables

```{r}
#Build the new model
logitModelNew <- stepAIC(logitModelFull,trace = 0) # trace = 0, as you do not want to get an output for the whole model selection process.

#Look at the model
summary(logitModelNew) 

# Save the formula of the new model (it will be needed for the out-of-sample part) 
formulaLogit <- as.formula(summary(logitModelNew)$call)
formulaLogit
```

```{r}
# Make predictions using the full Model
defaultData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)

# Construct the in-sample confusion matrix
confMatrixModelFull <- confusion.matrix(defaultData$PaymentDefault, defaultData$predNew, threshold = 0.5)
confMatrixModelFull
```

#### Finding the optimal threshold
Imagine you are running a campaign with the aim of preventing customers to default. You can lay out your campaign with the help of your predictions. Thereby, the choice of the threshold is essential for your results. If you know the costs and the rewards of your campaign, you can empirically check which threshold is most reasonable.

If a customer does not default due to our campaign, i.e. if we predicted the default correctly (true positive) we are rewarded with 1000€. If however we aim our campaign at a customer who would not have defaulted anyways, i.e. if we falsely predicted the customer (false positive) to default, we are faced with costs of 250€.

```{r optimal threshold with expected value}
threshold <- c(0.1, 0.2, 0.3, 0.4, 0.5)
payoff <- c(NA, NA, NA, NA, NA)
payoffMatrix <- data.frame(threshold, payoff)


for(i in 1:5) {
  # Calculate specific confusion matrix with respective threshold
  confMatrix <- confusion.matrix(defaultData$PaymentDefault,
                defaultData$predNew, 
                threshold = payoffMatrix$threshold[i])
  # Calculate TP - FPA
  payoffMatrix$payoff[i] <- 1000*confMatrix[2,2] - 250*confMatrix[2,1]
}
payoffMatrix
```

#### Train-test
```{r}
# Split data in train and test set
set.seed(534381) 
defaultData$isTrain <- rbinom(nrow(defaultData), 1, 0.66)
train <- subset(defaultData, isTrain == 1)
test <- subset(defaultData, isTrain  == 0)

logitTrainNew <- glm(formulaLogit, family = binomial, data = train) # Modeling
test$predNew <- predict(logitTrainNew, type = "response", newdata = test) # Predictions

# Out-of-sample confusion matrix and accuracy
confMatrixModelNew <- confusion.matrix(test$PaymentDefault, test$predNew, threshold = 0.3) 
sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew) # Compare this value to the in-sample accuracy
```

#### Cross Validation
Cross validation is a clever method to avoid overfitting 
```{r}
# Accuracy function
costAcc <- function(r, pi = 0) {
  cm <- confusion.matrix(r, pi, threshold = 0.3)
  acc <- sum(diag(cm)) / sum(cm)
  return(acc)
}

# Cross validated accuracy for logitModelNew
set.seed(534381)
cv.glm(defaultData, logitModelNew, cost = costAcc, K = 6)$delta[1]

```


## Modeling Time to Reorder with Survival Analysis


#### Survival Analysis
Data about customers of an online shop in order to practice survival analysis. But now it's not about the time until churn, but about the time until the second order. There are more who bought the second time.

In survival analysis, each observation has one of two states: either an event occured, or it didn't occur. But you don't know if it occurs tomorrow or in three years.
```{r}
# Plot a histogram
ggplot(dataNextOrder) +
  geom_histogram(aes(x = daysSinceFirstPurch,
                     fill = factor(boughtAgain), bins = 100)) +
  facet_grid( ~ boughtAgain) + # Separate plots for boughtAgain = 1 vs. 0
  theme(legend.position = "none") # Don't show legend

```


The hazard rate describes the risk that an event occurs within a very small period of time (provided that it hasn't occured yet).The hazard rate can go up and down. For example, customers could be very likely to churn at the beginning of their customer relationship, then become less likely for some months, and then become more likely again due to a saturation effect.

The cumulative hazard function describes the cumulative risk until time t.

For each observation there is the time under observation, marked with a + if the second order has not been placed yet.
```{r}
# Create survival object
survObj <- Surv(dataNextOrder$daysSinceFirstPurch, dataNextOrder$boughtAgain)
```

#### Kaplan-Meier: Survival Analysis
But now, the data contains an additional covariate called voucher, which you will need in this exercise. This categorical variable tells you if the customer used a voucher in her first order. It contains the value 0 or 1.

The survival function describes the proportion of observations who are still alive (or, for example, in a customer relationship, the proportion of customers who haven't churned yet), depending on their time under observation.
```{r}
# Compute and print fit
fitKMSimple <- survfit(survObj ~ 1)
print(fitKMSimple)

# Plot fit
plot(fitKMSimple, conf.int = FALSE,
     xlab = "Time since first purchase", ylab = "Survival function", main = "Survival function")

# Compute fit with covariate
fitKMCov <- survfit(survObj ~ voucher, data = dataNextOrder)

# Plot fit with covariate and add labels
plot(fitKMCov, lty = 2:3,
     xlab = "Time since first purchase", ylab = "Survival function", main = "Survival function")
legend(90, .9, c("No", "Yes"), lty = 2:3)


```


#### Hazard Function
Hazard assumes the influence of the predictors does not change over time.on the survival time after a short time under observation, but a large negative effect after a longer time under observation.

```{r}
# Determine distributions of predictor variables
dd <- datadist(dataNextOrder)
options(datadist = "dd")

# Compute Cox PH Model and print results
fitCPH <- cph(Surv(daysSinceFirstPurch, boughtAgain) ~ shoppingCartValue + voucher + returned + gender,
              data = dataNextOrder,
              x = TRUE, y = TRUE, surv = TRUE)
print(fitCPH)

# Interpret coefficients
exp(fitCPH$coefficients)

# Plot result summary
plot(summary(fitCPH), log = TRUE)
```

> You can see that a shopping cart value increase of 1 dollar decreases the hazard to buy again by a factor of only slightly below 1, but the coefficient is significant.

For customers who used a voucher, the hazard is 0.74 times lower, and for customers who returned any of the items, the hazard is 0.73 times lower. Being a man compared to a woman increases the hazard of buying again by the factor 1.11.


#### Evaluating Model assumptions

```{r}
# Check proportional hazard assumption and print result
testCPH <- cox.zph(fitCPH)
print(testCPH)

# The assumption seems to be violated for one variable at the 0.05 alpha level. Which one? Plot the coefficient beta dependent on time for this variable.

# Plot time-dependent beta
plot(testCPH, var = "gender") # supposed to be "gender=male"

# Validate model
validate(fitCPH, method = "crossvalidation",
         B = 10, dxy = TRUE, pr = FALSE)
```

Now you are going to predict the survival curve for a new customer from the Cox Proportional Hazard model you estimated before. The new customer is female and used a voucher in her first order (voucher = 1). The order was placed 21 days ago and had a shopping cart value of 99.90 dollars. She didn't return the order (returned = 0).

You are informed that due to database problems the gender was incorrectly coded: The new customer is actually male. The dataframe newCustomer is copied into a dataframe called newCustomer2 for you. Now go ahead and change the respective variable to male.
Recompute the predicted median with the corrected data. What changed?

Adding a customer and check their survival
```{r}
# Create data with new customer
newCustomer <- data.frame(daysSinceFirstPurch = 21, shoppingCartValue = 99.9, gender = "female", voucher = 1, returned = 0, stringsAsFactors = FALSE)

# Make predictions
pred <- survfit(fitCPH, newdata = newCustomer)
print(pred)
plot(pred)

# Dataset is copied. Now correct the customer's gender
newCustomer2 <- newCustomer
newCustomer2$gender <- "male"

# Redo prediction
pred2 <- survfit(fitCPH, newdata = newCustomer2)
print(pred2)
plot(pred2)
```
The correction of the gender decreased the predicted median time until the second order from 47 to 44 days.


## Reducing Dimensionality with Principal Component Analysis

Not only does this help to get a better understanding of your data. PCA also enables you to condense information to single indices and to solve multicollinearity problems in a regression analysis with many intercorrelated variables.

```{r}
# Overview of data structure:
str(newsData, give.attr = FALSE)

# Correlation structure:
# select(newsData, -url) %>% cor() %>% corrplot()
```
> The bottom right are highly correlated


Principal components are extracted such that they cover as much of the original variance as possible. If some variables had larger variance than others, they would be overrepresented in the components. Therefore standardising is important.


#### Compute a PCA
```{r}
# Standardize data
newsData <- select(newsData, -url) %>% scale() %>% as.data.frame()

# Compute PCA
pcaNews <- newsData %>% prcomp()

# Eigenvalues
(pcaNews$sdev)^2
```

#### Deciding how many components are relevant
```{r}
# Screeplot:
screeplot(pcaNews)

# Cumulative explained variance:
summary(pcaNews) # The Cumulative Proportion in the summary() of your PCA results gives you some insights.

# Kaiser-Guttmann (number of components with eigenvalue larger than 1):
sum((pcaNews$sdev)^2 > 1) # The Kaiser-Guttmann criterion considers the eigenvalues, i.e. the squared standard deviations of the components.
```
> The screeplot suggests 4 or 6 components. To explain 70% of the variance, you need as many as 10 components. The Kaiser-Guttmann criterion suggests 8 components, but the eigenvalues of PC7 and PC8 are already very close to 1. Therefore, 6 would be a good compromise.


#### Naming the components
```{r}
# Print loadings of the first component
pcaNews$rotation[, 1] %>% round(2)

# Print loadings of the first six components
pcaNews$rotation[, 1:6] %>% round(2)
```
> PC1 reflects “Subjectivity” (high global_subjectivity and avg_positive_polarity, negative loading on avg_negative_polarity). PC2 contains “Positivity” (high global_sentiment_polarity, low global_rate_negative_words; even negative words are not very negative as you can see from the positive loading on avg_negative_polarity). 

#### Visualization of PCA results with a biplot

```{r}
pcaNews %>% biplot(cex = 0.5)
```
> You can see a separated small group of articles with low values on PC1 and low variance in their PC2 values. These articles have a low subjectivity and are neither positive nor negative (which makes sense, because a very positive or negative article would probably also be higher in subjectivity).



#### Principal components in a regression analysis

logShares. The number of shares tell you how often the news articles have been shared. This distribution, however, would be highly skewed, so you are going to work with the logarithm of the number of shares.

```{r, include= FALSE}

newsData$logShares <- exp(newsData$shares) # wrong approach

# Predict log shares with all original variables
mod1 <- lm(logShares ~ ., newsData)

# Create dataframe with log shares and first 6 components
dataNewsComponents <- cbind(logShares = newsData[, "logShares"],
                            pcaNews$x[, 1:6]) %>%
  as.data.frame()

# Predict log shares with first six components
mod2 <- lm(logShares ~ ., dataNewsComponents)

# Print adjusted R squared for both models
summary(mod1)$adj.r.squared
summary(mod2)$adj.r.squared
```
> The R squared decreased only from 8% to 5% although the number of variables decreased from 21 to 6. However, even 8% of variance explained is not very good. That means, in order to get a good prediction for the (log) number of shares, you would probably need to collect additional variables or compute new variables from the existing ones.


