rm(list = ls()) # cleans the memory
library(tidyverse)
library(corrplot)
library(rms)
library(MASS)
library(SDMTools)
library(boot)
library(stats)
salesData <- read.csv('salesData.csv')
salesData2_4 <- read.csv('salesDataMon2To4.csv')
defaultData <- read.csv('defaultData.csv', sep=";")
dataNextOrder <- read.csv('survivalDataExercise.csv', sep=',')
newsData <- get(load('newsData.RData'))
salesModel1 <- lm(salesThisMon ~ . - id, data = salesData) # multiregression
vif(salesModel1) # Checking variance inflation factors
salesModel2 <- lm(salesThisMon ~ . - id - preferredBrand - nBrands, data = salesData) # Estimating new model by removing information on brand
vif(salesModel2) # Checking variance inflation factors
predSales5 <- predict(salesModel2, newdata = salesData2_4)
mean(predSales5)
summary(defaultData) # Summary of data
str(defaultData) # Look at data structure
# Analyze the balancedness of dependent variable
ggplot(defaultData,aes(x = PaymentDefault)) +
geom_histogram(stat = "count")
# Build logistic regression model
logitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage +
age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 +
billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 +
payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6,
family = binomial, data = defaultData)
summary(logitModelFull) # Take a look at the model
coefsexp <- coef(logitModelFull) %>% exp() %>% round(2) # extract the coefficients and transform them to the odds ratios.
coefsexp
#Build the new model
logitModelNew <- stepAIC(logitModelFull,trace = 0) # trace = 0, as you do not want to get an output for the whole model selection process.
#Look at the model
summary(logitModelNew)
# Save the formula of the new model (it will be needed for the out-of-sample part)
formulaLogit <- as.formula(summary(logitModelNew)$call)
formulaLogit
# Make predictions using the full Model
defaultData$predNew <- predict(logitModelNew, type = "response", na.action = na.exclude)
# Construct the in-sample confusion matrix
confMatrixModelFull <- confusion.matrix(defaultData$PaymentDefault, defaultData$predNew, threshold = 0.5)
confMatrixModelFull
threshold <- c(0.1, 0.2, 0.3, 0.4, 0.5)
payoff <- c(NA, NA, NA, NA, NA)
payoffMatrix <- data.frame(threshold, payoff)
for(i in 1:5) {
# Calculate specific confusion matrix with respective threshold
confMatrix <- confusion.matrix(defaultData$PaymentDefault,
defaultData$predNew,
threshold = payoffMatrix$threshold[i])
# Calculate TP - FPA
payoffMatrix$payoff[i] <- 1000*confMatrix[2,2] - 250*confMatrix[2,1]
}
payoffMatrix
# Split data in train and test set
set.seed(534381)
defaultData$isTrain <- rbinom(nrow(defaultData), 1, 0.66)
train <- subset(defaultData, isTrain == 1)
test <- subset(defaultData, isTrain  == 0)
logitTrainNew <- glm(formulaLogit, family = binomial, data = train) # Modeling
test$predNew <- predict(logitTrainNew, type = "response", newdata = test) # Predictions
# Out-of-sample confusion matrix and accuracy
confMatrixModelNew <- confusion.matrix(test$PaymentDefault, test$predNew, threshold = 0.3)
sum(diag(confMatrixModelNew)) / sum(confMatrixModelNew) # Compare this value to the in-sample accuracy
# Accuracy function
costAcc <- function(r, pi = 0) {
cm <- confusion.matrix(r, pi, threshold = 0.3)
acc <- sum(diag(cm)) / sum(cm)
return(acc)
}
# Cross validated accuracy for logitModelNew
set.seed(534381)
cv.glm(defaultData, logitModelNew, cost = costAcc, K = 6)$delta[1]
# Plot a histogram
ggplot(dataNextOrder) +
geom_histogram(aes(x = daysSinceFirstPurch,
fill = factor(boughtAgain), bins = 100)) +
facet_grid( ~ boughtAgain) + # Separate plots for boughtAgain = 1 vs. 0
theme(legend.position = "none") # Don't show legend
# Create survival object
survObj <- Surv(dataNextOrder$daysSinceFirstPurch, dataNextOrder$boughtAgain)
# Compute and print fit
fitKMSimple <- survfit(survObj ~ 1)
print(fitKMSimple)
# Plot fit
plot(fitKMSimple, conf.int = FALSE,
xlab = "Time since first purchase", ylab = "Survival function", main = "Survival function")
# Compute fit with covariate
fitKMCov <- survfit(survObj ~ voucher, data = dataNextOrder)
# Plot fit with covariate and add labels
plot(fitKMCov, lty = 2:3,
xlab = "Time since first purchase", ylab = "Survival function", main = "Survival function")
legend(90, .9, c("No", "Yes"), lty = 2:3)
# Determine distributions of predictor variables
dd <- datadist(dataNextOrder)
options(datadist = "dd")
# Compute Cox PH Model and print results
fitCPH <- cph(Surv(daysSinceFirstPurch, boughtAgain) ~ shoppingCartValue + voucher + returned + gender,
data = dataNextOrder,
x = TRUE, y = TRUE, surv = TRUE)
print(fitCPH)
# Interpret coefficients
exp(fitCPH$coefficients)
# Plot result summary
plot(summary(fitCPH), log = TRUE)
# Check proportional hazard assumption and print result
testCPH <- cox.zph(fitCPH)
print(testCPH)
# The assumption seems to be violated for one variable at the 0.05 alpha level. Which one? Plot the coefficient beta dependent on time for this variable.
# Plot time-dependent beta
plot(testCPH, var = "gender") # supposed to be "gender=male"
# Validate model
validate(fitCPH, method = "crossvalidation",
B = 10, dxy = TRUE, pr = FALSE)
# Create data with new customer
newCustomer <- data.frame(daysSinceFirstPurch = 21, shoppingCartValue = 99.9, gender = "female", voucher = 1, returned = 0, stringsAsFactors = FALSE)
# Make predictions
pred <- survfit(fitCPH, newdata = newCustomer)
print(pred)
plot(pred)
# Dataset is copied. Now correct the customer's gender
newCustomer2 <- newCustomer
newCustomer2$gender <- "male"
# Redo prediction
pred2 <- survfit(fitCPH, newdata = newCustomer2)
print(pred2)
plot(pred2)
# Overview of data structure:
str(newsData, give.attr = FALSE)
# Correlation structure:
# select(newsData, -url) %>% cor() %>% corrplot()
# Standardize data
newsData <- select(newsData, -url) %>% scale() %>% as.data.frame()
newsData
# Standardize data
newsData <- select(newsData, -url) %>% scale() %>% as.data.frame()
# Standardize data
newsData <- select(newsData, -url) %>% scale() %>% as.data.frame()
newsData
# Standardize data
newsData <- newsData %>% select(-url) %>% scale() %>% as.data.frame()
# Standardize data
newsData <- newsData %>% select(-url) %>% scale()
newsData
# Standardize data
newsData <- newsData %>% select(-url) %>% scale() %>% as.data.frame()
rm(list = ls()) # cleans the memory
library(tidyverse)
library(DataExplorer)
library(mlogit)
# ----- import data
sportscar <- read.csv('sportscar_choice_long.csv')
sportscar_wide <- read.csv('sportscar_choice_wide.csv')
chocolate_wide <- read.csv('chocolate_choice_wide.csv')
chocolate_long <- read.csv('chocolate_choice_long.csv')
# ----- data cleaning
sportscar$seat <- as.factor(sportscar$seat)
sportscar$choice_log <- as.logical(sportscar$choice)
# ----- EDA
str(sportscar)
summary(sportscar)
plot_histogram(sportscar)
pairs(sportscar, panel=panel.smooth)
plot_correlation(sportscar)
# Create a table of chosen sportscars by transmission type
chosen_by_trans <- xtabs(choice ~ trans, data = sportscar) # sum up the choice variable for each level of the trans variable.
# Print the chosen_by_trans table to the console
chosen_by_trans
# Plot the chosen_by_trans object
barplot(chosen_by_trans)
# fit a choice model using mlogit() and assign the output to m1
m1 <- mlogit(choice ~ seat + trans + convert + price, data=sportscar)
sportscar
sportscar$choice
sportscar$seat
sportscar$price
sportscar$convert
sportscar
sportscar$seat
sportscar$trans
sportscar$convert
sportscar$price
sportscar
nrow(chocolate_wide)
head(chocolate_wide)
chocolate_wide %>% filter(Subject == 2408, Trial == 3)
# use reshape() to change the data from wide to long
chocolate <- reshape(data= chocolate_wide , direction="long",
varying = list(Brand=3:5, Price=6:8, Type=9:11),
v.names=c("Brand", "Price", "Type"), timevar="Alt")
# Create the new order for the chocolate data frame
new_order <- order(chocolate$Subject, chocolate$Trial, chocolate$Alt)
# Reorder the chocolate data frame to the new_order
chocolate <- chocolate[new_order,]
# Transform the Selection variable to a logical indicator
chocolate$Selection <- chocolate$Alt == chocolate$Selection
# Look at the head() of chocolate to see how it has been reordered
head(chocolate)
# Use xtabs to count up how often each Type is chosen
counts_type <- xtabs(~ Type + Selection, data=chocolate)
counts_brand<- xtabs(~ Brand + Selection, data=chocolate)
counts_price <- xtabs(~ Price + Selection, data=chocolate)
# Plot the counts
plot(counts_type, cex = 1.5)
plot(counts_brand, cex = 1.5)
plot(counts_price, cex=1.5)
# use mlogit.data() to convert chocolate_df to mlogit.data
chocolate <- mlogit.data(chocolate_long, shape = "long",
choice = "Selection", alt.var = "Alt",
varying = 6:8)
# use str() to confirm that chocolate is an mlogit.data object
names(chocolate)
# Fit a model with mlogit() and assign it to choc_m
choc_m <- mlogit(Selection ~ 0 + Brand + Type + Price, data=chocolate, print.level = 3) # 0 removes intercept
# Summarize choc_m with summary()
summary(choc_m)
# compute the wtp by dividing the coefficient vector by the negative of the price coefficient
coef(choc_m)/-coef(choc_m)[9]
chocolate_long
